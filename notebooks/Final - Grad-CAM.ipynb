{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join, exists\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../..'))\n",
    "from project.dataloaders.prna_dataloader import PrnaDataLoader\n",
    "from project.models.prna import CTN\n",
    "from project.utils.waveform_utils import WAVEFORM_SAMPLE_RATES, apply_filter, normalize\n",
    "from project.utils.runners import train_mlp, test_mlp_single_fold, test_seq_mlp\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import pickle\n",
    "import biosppy\n",
    "\n",
    "# Transformer parameters\n",
    "# Copied from the PRNA model\n",
    "#\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "d_model = 256   # embedding size\n",
    "nhead = 8       # number of heads\n",
    "d_ff = 2048     # feed forward layer size\n",
    "num_layers = 8  # number of encoding layers\n",
    "dropout_rate = 0.2\n",
    "model_name = 'ctn'\n",
    "nb_demo = 2\n",
    "nb_feats = 20\n",
    "classes = sorted(['270492004', '164889003', '164890007', '426627000', '713427006',\n",
    "                  '713426002', '445118002', '39732003', '164909002', '251146004',\n",
    "                  '698252002', '10370003', '284470004', '427172004', '164947007',\n",
    "                  '111975006', '164917005', '47665007', '427393009',\n",
    "                  '426177001', '426783006', '427084000', '164934002',\n",
    "                  '59931005'])\n",
    "\n",
    "concept_to_desc = {\n",
    "    \"270492004\": \"first degree atrioventricular block\",\n",
    "    \"164889003\": \"atrial fibrillation\",\n",
    "    \"426627000\": \"bradycardia\",\n",
    "    \"164890007\": \"atrial flutter\",\n",
    "    \"713427006\": \"complete right bundle branch block\",\n",
    "    \"713426002\": \"incomplete right bundle branch block\",\n",
    "    \"445118002\": \"left anterior fascicular block\",\n",
    "    \"39732003\": \"left axis deviation\",\n",
    "    \"164909002\": \"left bundle branch block\",\n",
    "    \"251146004\": \"low QRS voltage\",\n",
    "    \"698252002\": \"non-specific intraventricular conduction delay\",\n",
    "    \"10370003\": \"Pacing rhythm\",\n",
    "    \"284470004\": \"Premature atrial contraction\",\n",
    "    \"427172004\": \"Premature ventricular contractions\",\n",
    "    \"164947007\": \"Prolonged PR interval\",\n",
    "    \"111975006\": \"Prolonged QT interval\",\n",
    "    \"164917005\": \"Q wave abnormal\",\n",
    "    \"47665007\": \"Right axis deviation\",\n",
    "    \"427393009\": \"Sinus arrhythmia\",\n",
    "    \"426177001\": \"Sinus bradycardia\",\n",
    "    \"426783006\": \"Sinus rhythm\",\n",
    "    \"427084000\": \"Sinus tachycardia\",\n",
    "    \"164934002\": \"T wave abnormal\",\n",
    "    \"59931005\": \"T wave inversion\",\n",
    "    \"59118001\": \"Right bundle branch block (disorder)\",\n",
    "    \"63593006\": \"Supraventricular premature beats\",\n",
    "    \"17338001\": \"Ventricular premature beats\"\n",
    "}\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add position encodings to embeddings\n",
    "        # x: embedding vects, [B x L x d_model]\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    '''\n",
    "    Transformer encoder processes convolved ECG samples\n",
    "    Stacks a number of TransformerEncoderLayers\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d_model, h, d_ff, num_layers, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.d_ff = d_ff\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.pe = PositionalEncoding(d_model, dropout=0.1)\n",
    "\n",
    "        encode_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.h,\n",
    "            dim_feedforward=self.d_ff,\n",
    "            dropout=self.dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encode_layer, self.num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.permute(0, 2, 1)\n",
    "        out = self.pe(out)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = self.transformer_encoder(out)\n",
    "        out = out.mean(0)  # global pooling\n",
    "        return out\n",
    "\n",
    "\n",
    "# 15 second model\n",
    "class CTN(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, num_layers, dropout_rate, deepfeat_sz, nb_feats, nb_demo, classes):\n",
    "        super(CTN, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(  # downsampling factor = 20\n",
    "            nn.Conv1d(1, 128, kernel_size=14, stride=3, padding=2, bias=False),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(128, 256, kernel_size=14, stride=3, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, d_model, kernel_size=10, stride=2, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=10, stride=2, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=10, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=10, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.transformer = Transformer(d_model, nhead, d_ff, num_layers, dropout=0.1)\n",
    "        self.fc1 = nn.Linear(d_model, deepfeat_sz)\n",
    "        # self.fc2 = nn.Linear(deepfeat_sz+nb_feats+nb_demo, len(classes))\n",
    "        self.fc2 = nn.Linear(deepfeat_sz, len(classes))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        def _weights_init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # self.apply(_weights_init)\n",
    "\n",
    "    def forward(self, x, wide_feats):\n",
    "        z = self.encoder(x)  # encoded sequence is batch_sz x nb_ch x seq_len\n",
    "        out = self.transformer(z)  # transformer output is batch_sz x d_model\n",
    "        out = self.dropout(F.relu(self.fc1(out)))\n",
    "        # out = self.fc2(torch.cat([wide_feats, out], dim=1))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(model_loc, deepfeat_sz, remove_last_layer=False):\n",
    "    model = CTN(d_model, nhead, d_ff, num_layers, dropout_rate, deepfeat_sz, nb_feats, nb_demo, classes).to(device)\n",
    "    checkpoint = torch.load(model_loc, map_location=torch.device('cpu'))\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print('Loading best model: best_loss', checkpoint['best_loss'], 'best_auroc', checkpoint['best_auroc'],\n",
    "          'at epoch',\n",
    "          checkpoint['epoch'])\n",
    "\n",
    "    if remove_last_layer:\n",
    "        model = torch.nn.Sequential(*(list(list(model.children())[0].children())[:-2]))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-existing Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model: best_loss 0.10535395583685707 best_auroc tensor(0.8580) at epoch 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CTN(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv1d(1, 128, kernel_size=(14,), stride=(3,), padding=(2,), bias=False)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv1d(128, 256, kernel_size=(14,), stride=(3,), bias=False)\n",
       "      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv1d(256, 256, kernel_size=(10,), stride=(2,), bias=False)\n",
       "      (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): Conv1d(256, 256, kernel_size=(10,), stride=(2,), bias=False)\n",
       "      (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv1d(256, 256, kernel_size=(10,), stride=(1,), bias=False)\n",
       "      (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (14): ReLU(inplace=True)\n",
       "      (15): Conv1d(256, 256, kernel_size=(10,), stride=(1,), bias=False)\n",
       "      (16): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): ReLU(inplace=True)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (pe): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer_encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (3): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (5): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (6): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (7): TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (fc2): Linear(in_features=64, out_features=24, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "thrs = np.loadtxt(\"/deep/u/tomjin/aihc-aut20-selfecg/prna/outputs-wide-64-15sec-bs64/saved_models/ctn/fold_1/thrs.txt\")\n",
    "model = load_best_model(\"/deep/u/tomjin/aihc-aut20-selfecg/prna/outputs-wide-64-15sec-bs64/saved_models/ctn/fold_1/ctn.tar\", 64)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13623384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(1, 128, kernel_size=(14,), stride=(3,), padding=(2,), bias=False)\n",
       "  (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Conv1d(128, 256, kernel_size=(14,), stride=(3,), bias=False)\n",
       "  (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Conv1d(256, 256, kernel_size=(10,), stride=(2,), bias=False)\n",
       "  (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): Conv1d(256, 256, kernel_size=(10,), stride=(2,), bias=False)\n",
       "  (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv1d(256, 256, kernel_size=(10,), stride=(1,), bias=False)\n",
       "  (13): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): Conv1d(256, 256, kernel_size=(10,), stride=(1,), bias=False)\n",
       "  (16): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(model.children())[0].children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 24])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.zeros((10, 1, 15 * 500)).cuda()\n",
    "model(input, None).shape # bs, 1, ECG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pywt\n",
    "import wfdb\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from wfdb import processing\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.signal import decimate, resample\n",
    "from biosppy.signals import ecg\n",
    "from biosppy.signals.tools import filter_signal\n",
    "\n",
    "import random\n",
    "\n",
    "class ECGWindowPaddingDataset(Dataset):\n",
    "    def __init__(self, df, window_length, nb_windows, src_path, cls=classes, lead=2):\n",
    "        ''' Return randome window length segments from ecg signal, pad if window is too large\n",
    "            df: trn_df, val_df or tst_df\n",
    "            window: ecg window length e.g 2500 (5 seconds)\n",
    "            nb_windows: number of windows to sample from record\n",
    "        '''\n",
    "        self.df = df\n",
    "        self.window_length = window_length\n",
    "        self.nb_windows = nb_windows\n",
    "        self.src_path = src_path\n",
    "        self.classes = cls\n",
    "        self.lead = lead\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get data\n",
    "        row = self.df.iloc[idx]\n",
    "        filename = str(self.src_path + \"/\" + (row.Patient + '.hea'))\n",
    "        data, hdr = load_challenge_data(filename, self.lead, self.window_length)\n",
    "\n",
    "        # Apply band pass filter\n",
    "        data = apply_filter(data, [3, 45])\n",
    "        data = normalize(data)\n",
    "\n",
    "        lbl = row[self.classes].values.astype(int)\n",
    "        \n",
    "        # Add just enough padding to allow window\n",
    "        seq_len = len(data)\n",
    "        pad = np.abs(np.min(seq_len - self.window_length, 0))\n",
    "        if pad > 0:\n",
    "            data = np.pad(data, (0, pad+1))\n",
    "            seq_len = len(data) # get the new length of the ecg sequence\n",
    "\n",
    "        starts = np.random.randint(seq_len - self.window_length + 1, size=self.nb_windows) # get start indices of ecg segment    \n",
    "        \n",
    "        ecg_segs = []\n",
    "        for start in starts:\n",
    "            a = data[start:start+self.window_length]\n",
    "            ecg_segs.append([a])\n",
    "        ecg_segs = np.array(ecg_segs)\n",
    "        return ecg_segs, lbl, hdr, filename\n",
    "\n",
    "\n",
    "def load_challenge_data(header_file, lead=2, window_length=7500):\n",
    "    with open(header_file, 'r') as f:\n",
    "        header = f.readlines()    \n",
    "    sampling_rate = int(header[0].split()[2])    \n",
    "    mat_file = header_file.replace('.hea', '.mat')\n",
    "    try:\n",
    "        x = loadmat(mat_file)\n",
    "        recording = np.asarray(x['val'], dtype=np.float64)[lead-1, :] # To use Lead III data, pass in lead=3\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        print(f\"Found mat_file that couldn't be loaded: {mat_file}\")\n",
    "        recording = np.zeros((window_length * 500, ))\n",
    "    \n",
    "    # Standardize sampling rate\n",
    "    if sampling_rate > 500:\n",
    "        recording = decimate(recording, int(sampling_rate / 500))\n",
    "    elif sampling_rate < 500:\n",
    "        recording = resample(recording, int(len(recording) * (500 / sampling_rate)))\n",
    "    \n",
    "    return recording, header\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import ttach as tta\n",
    "from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n",
    "from pytorch_grad_cam.utils.svd_on_activations import get_2d_projection\n",
    "\n",
    "\n",
    "class BaseCAM1D:\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 target_layer,\n",
    "                 use_cuda=False,\n",
    "                 reshape_transform=None):\n",
    "        self.model = model.eval()\n",
    "        self.target_layer = target_layer\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:\n",
    "            self.model = model.cuda()\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.activations_and_grads = ActivationsAndGradients(self.model, \n",
    "            target_layer, reshape_transform)\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        return self.model(input_img, None)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        raise Exception(\"Not Implemented\")\n",
    "\n",
    "    def get_loss(self, output, target_category):\n",
    "        loss = 0\n",
    "        for i in range(len(target_category)):\n",
    "            loss = loss + output[i, target_category[i]]\n",
    "        return loss\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor,\n",
    "                      target_category,\n",
    "                      activations,\n",
    "                      grads,\n",
    "                      eigen_smooth=False):\n",
    "        weights = self.get_cam_weights(input_tensor, target_category, activations, grads)\n",
    "        weighted_activations = weights[:, :, None] * activations\n",
    "        if eigen_smooth:\n",
    "#             cam = get_2d_projection(weighted_activations)\n",
    "            raise Exception(\"Not translated to 1D\")\n",
    "        else:\n",
    "            cam = weighted_activations.sum(axis=1)\n",
    "        return cam\n",
    "\n",
    "    def forward(self, input_tensor, target_category=None, eigen_smooth=False):\n",
    "\n",
    "        if self.cuda:\n",
    "            input_tensor = input_tensor.cuda()\n",
    "\n",
    "        output = self.activations_and_grads(input_tensor)\n",
    "\n",
    "        if type(target_category) is int:\n",
    "            target_category = [target_category] * input_tensor.size(0)\n",
    "\n",
    "        if target_category is None:\n",
    "            target_category = np.argmax(output.cpu().data.numpy(), axis=-1)\n",
    "        else:\n",
    "            assert(len(target_category) == input_tensor.size(0))\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        loss = self.get_loss(output, target_category)\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        activations = self.activations_and_grads.activations[-1].cpu().data.numpy()\n",
    "        grads = self.activations_and_grads.gradients[-1].cpu().data.numpy()\n",
    "\n",
    "        cam = self.get_cam_image(input_tensor, target_category, \n",
    "            activations, grads, eigen_smooth)\n",
    "\n",
    "        cam = np.maximum(cam, 0)\n",
    "\n",
    "        result = []\n",
    "        for img in cam:\n",
    "#             plt.plot(np.squeeze(img))\n",
    "#             plt.show()\n",
    "            \n",
    "            img = cv2.resize(img, (1, 7500))\n",
    "            img = img - np.min(img)\n",
    "            img = img / np.max(img)\n",
    "#             plt.plot(np.squeeze(img))\n",
    "#             plt.show()\n",
    "            result.append(img)\n",
    "        result = np.float32(result)\n",
    "        return result\n",
    "\n",
    "    def __call__(self,\n",
    "                 input_tensor,\n",
    "                 target_category=None,\n",
    "                 aug_smooth=False,\n",
    "                 eigen_smooth=False):\n",
    "\n",
    "        return self.forward(input_tensor,\n",
    "            target_category, eigen_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationsAndGradients:\n",
    "    \"\"\" Class for extracting activations and\n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layer, reshape_transform):\n",
    "        self.model = model\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        self.reshape_transform = reshape_transform\n",
    "\n",
    "        target_layer.register_forward_hook(self.save_activation)\n",
    "        target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        activation = output\n",
    "        if self.reshape_transform is not None:\n",
    "            activation = self.reshape_transform(activation)\n",
    "        self.activations.append(activation.cpu().detach())\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        # Gradients are computed in reverse order\n",
    "        grad = grad_output[0]\n",
    "        if self.reshape_transform is not None:\n",
    "            grad = self.reshape_transform(grad)\n",
    "        self.gradients = [grad.cpu().detach()] + self.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.gradients = []\n",
    "        self.activations = []        \n",
    "        return self.model(x, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_grad_cam.base_cam import BaseCAM\n",
    "\n",
    "class GradCAM1D(BaseCAM1D):\n",
    "    def __init__(self, model, target_layer, use_cuda=False, \n",
    "        reshape_transform=None):\n",
    "        super(GradCAM1D, self).__init__(model, target_layer, use_cuda, reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "#         print(f\"gradsshape: {grads.shape}\")\n",
    "        output_mean = np.mean(grads, axis=2)\n",
    "#         print(f\"output_mean = {output_mean.shape}\")\n",
    "#         print(f\"output_mean: {output_mean}\")\n",
    "        return output_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the CAM object once, and then re-use it on many images:\n",
    "cam = GradCAM1D(model=model, target_layer=list(list(model.children())[0].children())[0], use_cuda=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Full Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ecgs(correct_df, df_offsets, patient_ids=None, rows=30, show_labels=True):\n",
    "    \n",
    "    fig, ax = plt.subplots(rows, 3, figsize=(15, (45 / 30) * rows))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    ind = 0\n",
    "    for i, row in tqdm(correct_df.iterrows()):\n",
    "        patient_id = int(row[\"patient_id\"])\n",
    "        if patient_ids is not None and patient_id not in patient_ids:\n",
    "            continue\n",
    "\n",
    "        label = int(row[\"outcome\"])\n",
    "        actual = int(float(row[\"preds\"]) >= best_thresh)\n",
    "        actual_row = ind\n",
    "\n",
    "        df_offsets_sub = df_offsets[df_offsets[\"record_name\"] == patient_id]\n",
    "\n",
    "        start_offset_1 = int(df_offsets_sub.index[0])\n",
    "        start_offset_2 = int(df_offsets_sub.index[1])\n",
    "        start_offset_3 = int(df_offsets_sub.index[2])\n",
    "        ecg1 = ecg[start_offset_1, :]\n",
    "        ecg2 = ecg[start_offset_2, :]\n",
    "        ecg3 = ecg[start_offset_3, :]\n",
    "\n",
    "        ecg1_torch = torch.from_numpy(ecg1)\n",
    "        ecg1_torch = torch.reshape(ecg1_torch, (1, 1, -1))\n",
    "        ecg1_torch = ecg1_torch.float().cuda()\n",
    "        ecg2_torch = torch.from_numpy(ecg2)\n",
    "        ecg2_torch = torch.reshape(ecg2_torch, (1, 1, -1))\n",
    "        ecg2_torch = ecg2_torch.float().cuda()\n",
    "        ecg3_torch = torch.from_numpy(ecg3)\n",
    "        ecg3_torch = torch.reshape(ecg3_torch, (1, 1, -1))\n",
    "        ecg3_torch = ecg3_torch.float().cuda()\n",
    "\n",
    "        grayscale_cam_1 = cam(input_tensor=ecg1_torch)\n",
    "        grayscale_cam_2 = cam(input_tensor=ecg2_torch)\n",
    "        grayscale_cam_3 = cam(input_tensor=ecg3_torch)\n",
    "\n",
    "        grayscale_cam_1 = np.squeeze(grayscale_cam_1)[:500*3]\n",
    "        grayscale_cam_2 = np.squeeze(grayscale_cam_2)[:500*3]\n",
    "        grayscale_cam_3 = np.squeeze(grayscale_cam_3)[:500*3]\n",
    "\n",
    "        ecg1 = ecg1[:500*3]\n",
    "        ecg2 = ecg2[:500*3]\n",
    "        ecg3 = ecg3[:500*3]\n",
    "\n",
    "        ax[actual_row, 0].plot(ecg1)\n",
    "        ax[actual_row, 0].fill_between(range(500*3), ecg1 - grayscale_cam_1, ecg1 + grayscale_cam_1, color='#2e74e6', alpha=0.2)\n",
    "        ax[actual_row, 0].set_ylim([-1, 1])\n",
    "        ax[actual_row, 0].set_ylabel(patient_id)\n",
    "        ax[actual_row, 0].set_xlim([0, 500*3])\n",
    "        ax[actual_row, 0].set_xticks([])\n",
    "        ax[actual_row, 0].set_xticks([], minor=True)\n",
    "        ax[actual_row, 0].set_yticks([])\n",
    "        ax[actual_row, 0].set_yticks([], minor=True)\n",
    "\n",
    "        ax[actual_row, 1].plot(ecg2[:500*3])\n",
    "        ax[actual_row, 1].fill_between(range(500*3), ecg2 - grayscale_cam_2, ecg2 + grayscale_cam_2, color='#2e74e6', alpha=0.2)\n",
    "        ax[actual_row, 1].set_ylim([-1, 1])\n",
    "        ax[actual_row, 1].set_xlim([0, 500*3])\n",
    "        if show_labels:\n",
    "            ax[actual_row, 1].set_title(f\"Predicted={actual} | Actual={label}\")\n",
    "        ax[actual_row, 1].set_xticks([])\n",
    "        ax[actual_row, 1].set_xticks([], minor=True)\n",
    "        ax[actual_row, 1].set_yticks([])\n",
    "        ax[actual_row, 1].set_yticks([], minor=True)\n",
    "\n",
    "        ax[actual_row, 2].plot(ecg3[:500*3])\n",
    "        ax[actual_row, 2].fill_between(range(500*3), ecg3 - grayscale_cam_3, ecg3 + grayscale_cam_3, color='#2e74e6', alpha=0.2)\n",
    "        ax[actual_row, 2].set_ylim([-1, 1])\n",
    "        ax[actual_row, 2].set_xlim([0, 500*3])\n",
    "        ax[actual_row, 2].set_xticks([])\n",
    "        ax[actual_row, 2].set_xticks([], minor=True)\n",
    "        ax[actual_row, 2].set_yticks([])\n",
    "        ax[actual_row, 2].set_yticks([], minor=True)\n",
    "\n",
    "        if ind >= (rows - 1):\n",
    "            break\n",
    "        ind += 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg = np.load(f\"/deep/group/ed-monitor/patient_data_v9/waveforms/15sec-500hz-1norm-10wpp/II/waveforms.dat.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offsets = pd.read_csv(f\"/deep/group/ed-monitor/patient_data_v9/waveforms/15sec-500hz-1norm-10wpp/II/summary.csv\")\n",
    "df_offsets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find average threshold on validation set\n",
    "# See: Threshold notebook\n",
    "\n",
    "best_thresh = 0.096377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracies = 0.6998525073746312\n"
     ]
    }
   ],
   "source": [
    "# Apply threshold to test (all patients)\n",
    "\n",
    "df = pd.read_csv(f\"/deep/group/ed-monitor/patient_data_v9/predictions/15sec-500hz-1norm-10wpp/II/final-transformer-64/waveform-only/test.csv\")\n",
    "df_cohort = pd.read_csv(f\"/deep/group/ed-monitor/patient_data_v9/consolidated.filtered.test.txt\", sep=\"\\t\")\n",
    "df = pd.merge(df, df_cohort, on=\"patient_id\")\n",
    "testy = df[\"actual\"]\n",
    "yhat = df[\"preds\"]\n",
    "\n",
    "preds_t = (df[\"preds\"] >= best_thresh).to_numpy()\n",
    "accuracy = sum(preds_t == df[\"actual\"]) / len(df[\"actual\"])\n",
    "print(f\"Average accuracies = {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
